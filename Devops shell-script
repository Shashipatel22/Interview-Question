1 - Explain DevOps? 

>>The DevOps is a combination of two words one is software Development, and second is Operations. 
It allows a single team to handle the entire application lifecycle, from development to testing, deployment, and operations. 
DevOps helps you to reduce the disconnection between software developers, quality assurance (QA) engineers, and system administrators.

2 - Why has DevOps become famous?
As we know before DevOps there are two other software development models:

Waterfall model
Agile model
In the waterfall model, we have limitations of one-way working and lack of communication with customers. This was overcome in Agile by including
the communication between the customer and the company by taking feedback. But in this model, another issue is faced regarding communication between 
the Development team and operations team due to which there is a delay in the speed of production. This is where DevOps is introduced. It bridges the 
gap between the development team and the operation team by including the automation feature. Due to this, the speed of production is increased. 
By including automation, testing is integrated into the development stage. Which resulted in finding the bugs at the very initial stage which increased 
the speed and efficiency.

3 - What are the principles of DevOps?
The principles behind DevOps are:

Continuous deployment
Infrastructure as code
Automation
Monitoring
Security

4 - How DevOps is helpful to developers?
DevOps is very helpful for developers to fix the bugs and quickly implement the new features. It also helps in more transparent communication 
between the team members.

5 - Explain some popular tools of DevOps?
Here are some popular tools of DevOps, such as:

Jenkins:- Jenkins is a DevOps tool for monitoring the execution of repeated tasks. Jenkins is a software that allows continuous integration. 
And it will be installed on a server where the central build will take place.

Ansible:- Ansible is a leading DevOps tool. Ansible is an open-source IT engine that automates application deployment, 
cloud provisioning, intra service orchestration, and other IT tools.

Nagios:- Nagios is one of the more useful tools for DevOps. It can determine the errors and rectify them with the help of network, infrastructure, 
server, and log monitoring systems.

Docker:- Docker is a high-end DevOps tool that allows building, ship, and run distributed applications on multiple systems.

Git:- Git is an open-source distributed version control system that is freely available for everyone. It is designed to handle minor to major
projects with speed and efficiency.

6 - What are the prerequisites for the DevOps implementation?
Following are some useful prerequisites for DevOps implementation:

Proper communication between the team members.
At least one version control software.
Automated testing.
Automated deployment.

7 - What are the different phases in DevOps?
The various phases of the DevOps lifecycle are as follows:

Plan - Initially, there should be a plan for the type of application that needs to be developed. Getting a rough picture of the development process is always a good idea.
Code - The application is coded as per the end-user requirements. 
Build - Build the application by integrating various codes formed in the previous steps.
Test - This is the most crucial step of the application development. Test the application and rebuild, if necessary.
Integrate - Multiple codes from different programmers are integrated into one.
Deploy - Code is deployed into a cloud environment for further usage. It is ensured that any new changes do not affect the functioning of a high traffic website.
Operate - Operations are performed on the code if required.
Monitor - Application performance is monitored. Changes are made to meet the end-user requirements.

8 - What are the key components of DevOps?
The most important key components of DevOps are:

Continuous integration
Continuous testing
Continuous delivery
Continuous mongering

9 - Mention some of the core benefits of DevOps.
The core benefits of DevOps are as follows:

Technical benefits
Continuous software delivery
Less complex problems to manage
Early detection and faster correction of defects
Business benefits
Faster delivery of features
Stable operating environments
Improved communication and collaboration between the teams

10 - How is DevOps different from agile methodology?
DevOps is a culture that allows the development and the operations team to work together. This results in continuous development, testing, integration, 
deployment, and monitoring of the software throughout the lifecycle. 

11 - What are the fundamental differences between DevOps & Agile?

>>DevOps is a culture, fostering collaboration amongst all participants involved in the development and maintenance of software. 
Agile can be described as a development methodology designed to maintain productivity and drive releases with the common reality of changing needs.

12 - what is continous delivery and continous deployment 

---Continous delivery
>>Continuous delivery is a software development practice that uses automation to speed the release of new code. It establishes a process through which a developer's 
changes to an application can be pushed to a code repository or container registry through automation

---Continous deployment
>>Continuous deployment (CD, or CDE) is a strategy or methodology for software releases where any new code update or change made through the rigorous automated test 
process is deployed directly into the live production environment, where it will be visible to customers.


13 - What is the difference between continuous delivery and continuous deployment?

>>The difference between continuous delivery and continuous deployment is the presence of a manual approval to update to production. 
With continuous deployment, production happens automatically without explicit approval. Continuous delivery automates the entire software release process

14 - Which scripting tools are used in DevOps?

>>Both Python and Ruby scripting tools are used in the DevOps.

15 - Why open source tools boost DevOps?

>>Open source tools mainly used by any organization which is adapted by DevOps pipeline because DevOps came with the focus of automation 
in various aspects of organization build, release, change management, and infrastructure management areas.

16 - What is the role of AWS in DevOps?

>>AWS is a cloud-based service provided by Amazon that ensures scalability through unlimited computing power and storage. 
It empowers IT enterprises to develop and deliver experienced products and deploy applications on the cloud

17 - Explain two-factor authentication?
The two-factor authentication is a security method in which the user provides two ways of identification from separate categories.

18 - Name some network monitoring tools?

Some most essential network monitoring tools are:

Nagios
OpenNMS
Splunk
Icinga 2
Wireshark

19 - Which makes AWS DevOps highly accessible?
Here are some reasons which make AWS DevOps a highly popular, such as:

AWS CloudFormation
AWS EC2
AWS CloudWatch
AWS CodePipeline

20 - What is the Build in DevOps?
The build is a method in which the source code is put together to check whether it works as a single unit. 
In the build creation process, the source code will undergo compilation, testing, inspection, and deployment

21 -  What is Continuous Testing?
Continuous Testing constitutes the running of automated tests as part of the software delivery pipeline to provide 
instant feedback on the business risks present in the most recent release. In order to prevent problems in step-switching
in the Software delivery life-cycle and to allow Development teams to receive immediate feedback, every build is continually 
tested in this manner. This results in significant increase in speed in a developer's productivity as it eliminates the requirement 
for re-running all the tests after each update and project re-building

22 - What is Automation Testing?
Test automation or manual testing Automation is the process of automating a manual procedure in order to test an application or system.
Automation testing entails the use of independent testing tools that allow you to develop test scripts that can be run repeatedly without
the need for human interaction.

Helps to save money and time.
Unattended execution can be easily done.
Huge test matrices can be easily tested.
Parallel execution is enabled.

23 - what is infrastructure deployment

two ways to build infrastructure deployment

1-cloud native tools
a)-aws    -->cloud formation  yaml or json uses
b)-azure  -->arm templates
code is heavy in aws and azure

2-third party tools (terraform)
>>hashicorp tool it is an open-souce
>>it is famous it is easy to use and understand
>>it support to all clouds
>>terraform have idempotnce

24-what is configuration mangement

>>it is used to configure the servers or users any updates or use to regular operations

tools:-
1-chef
2-puppet
3-ansible
4-rundeck
5-salt

25 - what is montoring tools

>>A monitoring application tracks your app's hardware utilization, SLA status, platform performance, and user response times. Among the metrics, 
DevOps engineers can monitor here are server diagnostics, error logs, network traffic reports, historical statistics, and failure diagnostics

Infrastrucrure-Monitoring tool
1-nagios
2-Prometheus 
3-Zabbix
4-datadog
5-new-relic
6-grafana

Application-monitoring tool
1-ELK stack
2-splunk
3-suma-logic
4-solar-wind

26 - What is a ticketing tool?

>>A ticketing system is a management tool that processes and catalogs customer service requests. 
Tickets, also known as cases or issues, need to be properly stored alongside relevant user information. 
The ticketing system should be user-friendly for customer service representatives, managers, and administrators.

types:-
1-jira
2-ServiceNow.

27 - what is prometheus and grafana

>>Prometheus is a monitoring solution for storing time series data like metrics. Grafana allows to visualize the data stored in Prometheus (and other sources). 
This sample demonstrates how to capture NServiceBus metrics, storing these in Prometheus and visualizing these metrics using Grafana

28 - what is redis server could you explain briefly

>>What is Redis? Redis, which stands for Remote Dictionary Server, is a fast, open source, in-memory, key-value data store.
 The project started when Salvatore Sanfilippo, the original developer of Redis, wanted to improve the scalability of his Italian startup

29 - how you can manage facts

>>since we have big environment. we have redis server which we have hosted in the aws 
and we are managing all the facts in the redis for improved performance

30 - what is sonarqube and what is the use of it

>>SonarQube (formerly Sonar) is an open-source platform developed by SonarSource for 
continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs and code smells on 29 programming languages.
     or
>>SonarQube is a Code Quality Assurance tool that collects and analyzes source code, and provides reports for the code quality of your project. 
It combines static and dynamic analysis tools and enables quality to be measured continually over time

31 - what is maven&ant&graddle what is the use of it

>>Gradle is a Groovy-based build automation tool that is an open-source and builds based on the concepts of Apache Maven and Apache Ant. 
It is capable of building almost any type of software. It is designed for the multi-project build, which can be quite large

32 - what is jfrog artifacts and what is the use for it

>>JFrog Artifactory is a universal DevOps solution providing end-to-end automation and 
management of binaries and artifacts through the application delivery process that improves productivity across your development ecosystem

33 - what is artifactes 

>>artifact is a by-product produced during the software development process. 
It may consist of the project source code, dependencies, binaries or resources, and could be represented in different layout depending on the technology

34 - what is maven

>>Maven is a popular open-source build tool developed by the Apache Group to build, publish, and deploy several projects at once for better project management. 
The tool provides allows developers to build and document the lifecycle framework.

35 - can you explain maven-build cycle

>>Maven makes the day-to-day work of Java developers easier and helps with the building and running of any Java-based project. 
Maven Lifecycle: Below is a representation of the default Maven lifecycle and its 8 steps: Validate, Compile, Test, Package, Integration test, Verify, Install and Deploy






=======================================GITHUB==================================================================================================

1 - What is Git?
Git is a version control system for tracking changes in computer files and is used to help coordinate work among several people on a project while
tracking progress over time. In other words, it’s a tool that facilitates source code management in software development.

Git favors both programmers and non-technical users by keeping track of their project files. It enables multiple users to work together and handles
large projects efficiently.

Advantgaes:-
Faster release cycles
Easy team collaboration
Widespread acceptance 
Maintains the integrity of source code
Pull requests

Benfites:-
Data replication and redundancy are both possible.
It is a service with high availability.
There can only be one Git directory per repository.
Excellent network and disc performance are achieved.
On any project, collaboration is very simple.

2 - What is GitHub?

To provide Internet hosting for version control and software development, GitHub makes use of Git.

3 - Mention some popular Git hosting services.

GitHub
SourceForge
GitLab
Bitbucket

4 - Different types of version control systems

Local version control systems have a database that stores all file changes under revision control on disc in a special format.
Centralized version control systems have a single repository, from which each user receives their working copy.
Distributed version control systems contain multiple repositories, and different users can access each one with their working copy.

5 - What’s the difference between Git and GitHub?

Git:-
1-Git is a software
2-Git can be installed locally on the system
3-Provides a desktop interface called git GUI
4-It does not support user management features

github:-
1-GitHub is a service
2-GitHub is hosted on the web
3-Provides a desktop interface called GitHub Desktop.
4-Provides built-in user management

6 - What is a Git repository?

Git repository refers to a place where all the Git files are stored. These files can either be stored on the local repository or on the remote repository.

7 - What does the git push command do?

The Git push command is used to push the content in a local repository to a remote repository. After a local repository has been modified,
a push is executed to share the modifications with remote team members.

8 - What do you understand about the Git merge conflict?
A Git merge conflict is an event that occurs when Git is unable to resolve the differences in code between the two commits automatically. 

Git is capable of automatically merging the changes only if the commits are on different lines or branches.

9 - how to solve merge conflicts
>>Person A is the one who decides when to incorporate new changes from master, so Person A will perform the merge. Person A should certainly attempt to resolve merge conflicts on their own, 
but if any questions arise then both Person A and Person B should sit together and resolve the conflicts together

10 - what is merge and rebase
>>Git merge is a command that allows you to merge branches from Git. Git rebase is a command that allows developers to integrate changes from one branch to another. 
In Git Merge logs will be showing the complete history of the merging of commits.

11 - what is the differnce between merge and rebase

>>Merging is a safe option that preserves the entire history of your repository, while rebasing creates a linear history by moving your feature branch onto the tip of main 

12 - what is rebase
>>Rebasing is the process of moving or combining a sequence of commits to a new base commit. Rebasing is most useful and easily visualized in the context of a feature branching workflow.

13 - what is the differnce between merge and pull request
>>The git pull command first runs git fetch which downloads content from the specified remote repository. 
Then a git merge is executed to merge the remote content refs and heads into a new local merge commit. 
To better demonstrate the pull and merging process let us consider the following example.

14 - what is git reset hard???
>>Summary. To review, git reset is a powerful command that is used to undo local changes to the state of a Git repo. Git reset operates on "The Three Trees of Git". 
These trees are the Commit History ( HEAD ), the Staging Index, and the Working Directory.
                or
>>git reset --hard , which will completely destroy any changes and remove them from the local directory.

15 - what is git reset soft???
>>git reset --soft , which will keep your files, and stage all changes back automatically

16 - what is git stash
>>git stash temporarily shelves (or stashes) changes you've made to your working copy so you can work on something else, and then come back and re-apply them later on.

17 - Tell me the difference between git pull and git fetch?
Both of these commands will fetch any new commits from the remote repository, but they differ in how they handle these commits.

Git pull will merge the remote commits into the current branch, while git fetch will simply retrieve the commits and store them in the local repository. 
This means that if you have any uncommitted changes, git pull may result in merge conflicts, while git fetch will not

18 - Difference between “pull request” and “branch”?
“Pull request” is done when you feel like changing the developer’s change to another person's code branch. And “Branch” is just a separate version of code

19 - What is a branch in Git?
A branch is a way to isolate development work on a particular aspect of a project. When a branch is created, it diverges from the primary branch. It allows 
developers to work on a new feature or bug fix without affecting the main codebase.

20 -  What is a commit in Git?
A commit is a way to save changes to a branch. When a commit is made, a snapshot of the current state of the branch is created. 
This snapshot can be used to revert the branch to that state if necessary.

21 - what is trufflehog and how to use it
>>Now, on to the more popularly used scanning tool — truffleHog. This GitHub repository scanner will look into your commit 
history and spot anything that looks like a password or confidential information using regex and entropy.

=======================================================Jenkins===========================================================================================
                           
1 - what is jenkins what will do jenkins and what is highly available jenkins

>>Jenkins is an open-source automation tool written in Java with plugins built for continuous integration. Jenkins is used to build and test your 
software projects continuously making it easier for developers to integrate changes to the project, and making it easier for users to obtain a fresh build.

highly available jenkins:-
>>All Jenkins instances within the Region can access the Amazon EFS file system and data durably stored in multiple Availability Zones. 
If a single Availability Zone experiences an outage, the Jenkins file system is still accessible from other Availability Zones providing HA for the storage laye

Why Jenkins is important?
>>Jenkins is used today along the entire software development lifecycle, enabling the integration and automation of different processes, including building, testing, 
and deployment. Jenkins creates 'pipelines', which define the series of steps that the server will take to perform the required tasks.

>>jenkins is on open-source tool    (cloudbess is also a jenkins tools it is an enterprise level) 
>>jenkins is ci/cd tool continoud integration and continous deployment and deliver 
>>jenkins nothing can do without plugins
>>jenkins works on java programming
>>you can develop your own plugins and you can use that plugins

2 - what language is used in jenkins pipeline

>>Groovy programming language
Pipelines are Jenkins jobs enabled by the Pipeline (formerly called “workflow”) plugin and built with simple text scripts that use a Pipeline DSL (domain-specific language)
 based on the Groovy programming language

3 - what is the use of jenkins slave 

>>Slaves can run on a variety of operating systems. The job of a Slave is to do as they are told to, which involves executing build jobs dispatched by the Master


4 - what is on-demand-slave in jenkins

>>Slaves are Jenkins agents it's used to execute jobs scheduled by the master. But there we need lots of different slaves with their respective tools. 
For example, one slave is configured for maven to run a java application, one is for docker, and so on

5 - How many slaves can Jenkins have?
1-200 is the range you can get to without having to *heavily* tune GC or change some Jenkins hidden properties

6 - what is role-based-access-control in jenkins

>>The Role Strategy plugin is meant to be used from Jenkins to add a new role-based mechanism to manage users' permissions. Supported features. Creating global roles, 
such as admin, job creator, anonymous, etc., allowing to set Overall, Agent, Job, Run, View and SCM permissions on a global basis

7 - what is jenkins pipeline and types of pipeline

>>In Jenkins, a pipeline is a collection of events or jobs which are interlinked with one another in a sequence. 
It is a combination of plugins that support the integration and implementation of continuous delivery pipelines using Jenkins

Types:-
1-declartive
2-scripted

8 - what is the differnce between scripted pipeline and declartive pipeline

>>Declarative: Declarative pipeline supports the Environment block. Environment block helps to define global variables and load credentials into the pipeline. 
Scripted: The scripted pipeline does not support Environment block but you can define global variables and use credentials in a different way using Groovy.

2-declartive pipeline
>>we always preferd to use the declartive pipeline,bcoz of the advantages associated
1-complete pipeline validation before executing
2-if the pipeline is not good means if any syntax error means its not build the pipeline 
like scripted will build to stop the stage like we converted 4 stages the syntax error 
in 3 stage that stage will get an error in scripted pipeline
3-restart at one particular stage.
4-good when applying conditional statements when compared to scripted pipeline

9 - do you worked on the multi-branch pipeline and use case of multi-branch pipeline

>>yes i worked on multi-branch pipeline we have blue-ocean inplace, we are using blue-ocean  
so that the blue-ocean give very good graphical representation to manage the multi-branch pipline
we have like dev-env and prod-env when ever the commit is done it will be build 

10 - what is the use of active directory in jenkins

>>With this plugin, you can configure Jenkins to authenticate the username and the password through Active Directory. 
This plugin internally uses two very different implementations, depending on whether Jenkins is running on Windows or non-Windows and if you specify a domain.

11 - What is a CI/CD pipeline?
CI/CD Pipeline or Continuous Integration/ Continuous Delivery is considered the DevOps approach's backbone. 
The pipeline is responsible for building codes, running tests, and deploying new software versions.

12 - Name some of the useful plugins in Jenkins.
Some of the plugins in Jenkins include:

Maven 2 project
Amazon EC2
Copy artifact
Join
HTML publisher
Green Balls

13 - How can you create a backup and copy files in Jenkins?

Jenkins stores all the settings, builds scripts, and logs in the home directory. 
Then, if you want to create a backup of this Jenkins set up all you have to do is copy this directory. 
The job directory may also be copied to clone a job or rename the directory.

14 - Name some more continuous Integration tools other than Jenkins.
Some of the top continuous integration tools other than Jenkins are:

TeamCity
Travis CI
Go CD
Bamboo
GitLab CI
CircleCI
Codeship

15 - Assume that you have a pipeline. The first job that you performed was successful, but the second one failed.  What would you do now?

>>You don't have to worry, and you just have to restart the pipeline from the point where it failed by doing 'restart from stage.'

16 - Explain the process in which Jenkins works?
Here’s the process in which Jenkins works:

Jenkins checks changes in repositories regularly, and developers must secure their code regularly. 
Once the changes are defined, Jenkins detects them and uses them to prepare a new build.
After that, Jenkins will transverse through various stages in its usual pipeline. As one stage completes, the process will move further on to the next stage.
If a stage fails, the Jenkins build will stop there, and the software will email the team using it. When completed successfully, 
the code implements itself in the proper server so that testing begins.
After the successful testing phase, Jenkins shares the results with the team using it.

17 - What is Jenkinsfile? 

Jenkins file is a text file that has a definition of a Jenkins pipeline and is checked into the source control repository.
It enables code review and iteration on the pipeline. It also permits an audit trail for the pipeline.

18 - Differentiate between Maven, Ant, and Jenkins.

Maven:-
Build tool
Perform build operations 

ANT:-
Build tool
Perform build operations 

Jenkins:-
Continuous Integration tool
Jenkins may run unit tests and deploy applications

19 - Why is Jenkins used with Selenium?

Using Selenium allows Jenkins’s testing whenever there are any software changes or any changes in the environment. 
When the Selenium test suite is integrated with Jenkins, the testing part is also automated as part of the build process.

20 - what is the multi-branch pipeline.What is the process of making a Multibranch Pipeline in Jenkins?

>>What's a Jenkins Multibranch Pipeline? A multibranch job is simply a folder of pipeline jobs. For every branch you have, Jenkins will create a folder.
So instead of creating a pipeline job for each of the branches you have in a git repository, you could use a multibranch job.

>>To create a Multibranch Pipeline in Jenkins, follow the following steps:

Open the Jenkins dashboard and create a new item by clicking on 'new item'
Enter the project name and, from the options, select 'Multibranch pipeline'
Click on OK
pipeline

Source: https://www.jenkins.io/doc/book/pipeline/multibranch/

Then select the repository location, branch source (GitHub/Bitbucket), and add the branch source credentials.
Save the project
Now, Jenkins automatically creates new Multibranch Pipelines for repositories
Then to connect to the GitHub repo, we need the HookURL
To get this URL from the repository settings, add this HookURL to the Webhooks section
Once the jobs are created, Jenkins will automatically trigger the build

============================================================Docker=========================================================================================

1 - what is docker and conatiner how to use in docker containers

>>Docker is an open-source containerization platform. It is used to automate the deployment of any application, using lightweight, portable containers.


Features;-
Application agility
Developer productivity
Easy modeling
Operational efficiencies
Placement and affinity
Version control

--disadvantages:-
>>Docker is not good for application that requires rich GUI.
>>It is difficult to manage large amount of containers.
>>Docker does not provide cross-platform compatibility means if an application is designed to run in a Docker container on windows, then it cannot run on Linux Docker container.
>>container is process it can fail anytime
--Advantages
>>Caching a cluster of containers.
>>Flexible resource sharing.
>>Scalability - many containers can be placed in a single host.
>>Running your service on hardware that is much cheaper than standard servers.

2 - Name and explain the various Docker components.
The three main Docker components are:

Docker Client. Performs Docker build pull and run operations to open up communication with the Docker Host. 
The Docker command then employs Docker API to call any queries to run.
Docker Host. Contains Docker daemon, containers, and associated images. The Docker daemon establishes a connection with the Registry. 
The stored images are the type of metadata dedicated to containerized applications.
Registry. This is where Docker images are stored. There are two of them, a public registry and a private one. 
Docker Hub and Docker Cloud are two public registries available for use by anyone.

3 - why everyone using production environment in kubernets and why not using docker and docker swarm

>>The major difference between the platforms is based on complexity. Kubernetes is well suited for complex applications. 
On the other hand, Docker Swarm is designed for ease of use, making it a preferable choice for simple applications

>>kubernets have additional network plugins thats why using kubernets and this is this reason docker swarm not using
plugins ex:-panel and kolak 

4 - what is the problems in persistent storage in docker

>>One common challenge with many cloud-native technologies is persistent data storage. Containers, serverless functions, and apps deployed using an 
immutable infrastructure model don't typically offer a way to store data permanently because all internal data is destroyed when the application shuts down

5 - What is a container?

>>it is processing machine or servie runing on linux
>>Containers are deployed applications bundled with all necessary dependencies and configuration files. All of the elements share the same OS kernel. 
Since the container isn’t tied to any one IT infrastructure, it can run on a different system or the cloud.

6 - Explain virtualization.
Virtualization is the means of employing software (such as Hypervisor) to create a virtual version of a resource such as a server, data storage, or application. 
Virtualization lets you divide a system into a series of separate sections, each one acting as a distinct individual system. 
The virtual environment is called a virtual machine

7 - What’s the difference between virtualization and containerization?
Virtualization is an abstract version of a physical machine, while containerization is the abstract version of an application.

8 - Last simple question…Describe a Docker container’s lifecycle.
Although there are several different ways of describing the steps in a Docker container’s lifecycle, the following is the most common:

Create container
Run container
Pause container
Unpause container
Start container
Stop container
Restart container
Kill container
Destroy container

9 - Name the essential Docker commands and what they do.
The most critical Docker commands are:

Build. Builds a Docker image file
Commit. Creates a new image from container changes
Create. Creates a new container
Dockerd. Launches Docker daemon
Kill. Kills a container

10 - What are Docker object labels?

Labels are the mechanism for applying metadata to Docker objects such as containers, images, local daemons, networks, volumes, and nodes.

11 - what is nodes in docker 

>>A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, 
but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines

12 - what docker volumes and mounts explain briefly.How do you find stored Docker volumes?

>>Docker volumes are dependent on Docker's file system and are the preferred method of persisting data for Docker containers and services. When a container is started, Docker loads the read-only image layer, 
adds a read-write layer on top of the image stack, and mounts volumes onto the container filesystem

Use the command: /var/lib/docker/volumes

13 - can you write a docker file???

--vi Dockerfile
FROM ubuntu
LABEL  owner=kanji5413@gmail.com
RUN apt-get update; \
    apt install -y nginx vi nano wget curl net-tools; \
    mkdir /myapp 
RUN groupadd -r mongodb && useradd -r -g mongodb mongodb
WORKDIR /myapp
#CMD ["nginx", "-g", "daemon off;"]
USER mongodb

:wq

>>docker build .       -->. means present directory 

14 - what is docker compose and docker stack

>>docker compose will allow developer or devops person to create multiple containers and it will save 
lot time .and its not for production purpose it is only per tseting. if the container get stopped the 
web application will be face down-time. it is only for one server. if you want install multiple servers you need to use docker-stack

>>docker stack:-Docker Stack is run across a Docker Swarm, which is essentially a group of machines running the Docker daemon, which are grouped together,
essentially pooling resources. Stacks allow for multiple services, which are containers distributed across a swarm, to be deployed and grouped logically

15 - what is docker swarm 

>>A Docker Swarm is a group of either physical or virtual machines that are running the Docker application and that have been configured to join together in a cluster.
The activities of the cluster are controlled by a swarm manager, and machines that have joined the cluster are referred to as nodes.

>>Summary. The Docker Swarm mode allows an easy and fast load balancing setup with minimal configuration. Even though the swarm itself already performs 
a level of load balancing with the ingress mesh, having an external load balancer makes the setup simple to expand upon

16 - what is docker swarm is quoram 

>>In a swarm of N managers, a quorum (a majority) of manager nodes must always be available. For example, in a swarm with five managers,
 a minimum of three must be operational and in communication with each other

17 - why quoram needs an odd number
>>quoram is an latein word
>>minimum number of nodes in swarm cluster that needs to be up and running for swarm cluster to work

18 - what is docker ingress and docker gateway-bridge
>>The docker_gwbridge is a bridge network that connects the overlay networks (including the ingress network) to an individual Docker daemon's physical network. 
By default, each container a service is running is connected to its local Docker daemon host's docker_gwbridge network

19 - what is ingress controller
>>The Ingress Controller is an application that runs in a cluster and configures an HTTP load balancer according to Ingress resources. 
The load balancer can be a software load balancer running in the cluster or a hardware or cloud load balancer running externally

20 - what is roolingupdate in docker
>>The scheduler applies rolling updates as follows by default: Stop the first task. Schedule update for the stopped task. 
Start the container for the updated task. If the update to a task returns RUNNING , wait for the specified delay period then start the next task

21 - what is in efk stack in docker
>>EFK Stack is an enterprise-ready log aggregation and logs analysis framework for bare-metal and container infrastructure. 
But before deploying an EFK stack, you'll first set up a project directory and create a Docker configuration for deploying EFK Stack on your Docker host

22 - what is java memory leak 
>>A Memory Leak is a situation where there are objects present in the heap that are no longer used, but the garbage collector is unable to remove them from memory, 
and therefore, they're unnecessarily maintained.A memory leak is bad because it blocks memory resources and degrades system performance over time

>>How do you fix a memory leak in Java?

Using the java. lang. ref package, you can work with the garbage collector in your program. 
This allows you to avoid directly referencing objects and use special reference objects that the garbage collector easily clears.

23 - what does docker swarm do
1-worker node availibility
2-resource utilization
3-container scheduling
4-container fail-over or self-healing

24 - what is docker traefik ingress controller
>>Simplify networking, secure your APIs, and reduce the costs of managing your microservices with a dynamic, production-ready Kubernetes Ingress routing solution.

25 - what is docker node maintaince and types

>>A swarm consists of one or more nodes: physical or virtual machines running Docker Engine 1.12 or later in swarm mode. There are two types of nodes: managers and workers.

26 - what is docker swarm config

>>Docker swarm service configs allow you to store non-sensitive information, such as configuration files, outside a service's image or running containers. 
This allows you to keep your images as generic as possible, without the need to bind-mount configuration files into the containers or use environment variables

27 - what is docker swarm secrets

>>In terms of Docker Swarm services, a secret is a blob of data, such as a password, SSH private key, SSL certificate, or another piece of data that should not 
be transmitted over a network or stored unencrypted in a Dockerfile or in your application's source code

28 - what is docker swarm helath checks

>>he Docker swarm health check ensures that the Docker service is running properly. A faulty service can cause a major incident!

29 - In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, 
while providing isolation from containers which are not connected to that bridge network

30 - what is the differnce between registry and repository 

>>registry inside have an repository, repository inside you can put multiple images

32 - Types of registery

>>Docker hub , AWS ECR, Azure ACR ,google GCR

33 - how many types to run docker in AWS
install docler in ec2 and run conatiner
run using AWS Elastic conatiners service(ECS)
Run using AWS ECS Fargate
Run Using AWS EKS

==================================================================Ansible===============================================================================

1 - what is Ansible 

>>Ansible is an open-source platform that facilitates configuration management, task automation, or application deployment. It is a valuable DevOps tool. 
It was written in Python and powered by Red Hat. It uses SSH to deploy SSH without incurring any downtime
>>Ansible is Idempotent

2 - what is ansible host-patterns

>>An Ansible pattern can refer to a single host, an IP address, an inventory group, a set of groups, or all hosts in your inventory. 
Patterns are highly flexible - you can exclude or require subsets of hosts, use wildcards or regular expressions, and more. 
Ansible executes on all inventory hosts included in the pattern.

3 - Describe how Ansible works.

>>ansible is broken down into two types of servers: controlling machines and nodes. Ansible is installed on the controlling computer, 
and the controlling machines manage the nodes via SSH. 

The controlling machine contains an inventory file that holds the node system’s location. Ansible runs the playbook on the controlling machine 
to deploy the modules on the node systems. 
Since Ansible is agentless, there’s no need for a third-party tool to connect the nodes.

4 - Explain what a “playbook” is.

>>Ansible Playbooks are lists of tasks that automatically execute against hosts. Groups of hosts form your Ansible inventory. 
Each module within an Ansible Playbook performs a specific task. Each module contains metadata that determines when and where a 
task is executed, as well as which user executes it.

5 - What exactly is a configuration management tool?
Configuration management tools help keep a system running within the desired parameters. They help reduce deployment time and substantially 
reduce the effort required to perform repetitive tasks. Popular configuration management tools on the market today include Chef, Puppet, Salt, 
and of course, Ansible.

6 - What is Ansible Vault?

Ansible vault is used to keep sensitive data such as passwords instead of placing it as plaintext in playbooks or roles. 
Any structured data file or any single value inside the YAML file can be encrypted by Ansible.

ansible-vault encrypt foo.yml bar.yml baz.yml

ansible-vault decrypt foo.yml bar.yml baz.yml

7 - What is the ad-hoc command in Ansible?

>>there is no idempotant in Ad-hoc commnads
>>Ad-hoc commands are like one-line playbooks to perform a specific task only. The syntax for the ad-hoc command is

ansible [pattern] -m [module] -a "[module options]"
For example, we need to reboot all servers in the staging group

ansible atlanta -a "/sbin/reboot"  -u username --become [--ask-become-pass]

8 - What’s a handler?
In Ansible, a handler is similar to a regular task in a playbook, but it will only run if a task alerts the handler. 
Handlers are automatically loaded by roles/<role_name>/handlers/main.yaml. Handlers will run once, after all of the tasks are completed in a particular play. 

9 - what is Ansible Modules

>>What are Ansible modules used for?
A module is a reusable, standalone script that Ansible runs on your behalf, either locally or remotely. Modules interact with your local machine, an API,
or a remote system to perform specific tasks like changing a database password or spinning up a cloud instance.

==================================================================Terraform and packer=====================================================================================

1 - what is terraform

>>Terraform is a tool for building, changing and versioning infrastructure safely and efficiently. Terraform can manage existing and popular cloud service providers as
well as custom in-house solutions.Configuration files describe to Terraform the components needed to run a single application or your entire datacenter

2 - is it terraform cloud aganstics
>>terraform is cloud aganstic  -->yes terraform will work every cloud but the code is difference that why we calling cloud aganstics

3 - what is proprietory
it is like not a open-source when you using terraform you advised some code to then can verfiy and give resopnce 
but in aws your rasing an ticket but its take some time to contact and its not accept your advise this proprietory

4 - what is the differnce between argument refernce and attribute refernce in terraform
>>In Terraform v0. 12 and later, the language makes a distinction between argument syntax and nested block syntax within blocks: 
Argument syntax sets a named argument for the containing object. If the attribute has a default value then an explicitly-specified value entirely overrides that default.

5 - how to deploy dev environment and prod environment in a single folder 

>>dont use this type some times you may get confused but there is a way you need to create a workspaces
>>in a single folder you can use multiple workspaces using workspace
>>if your in dev environment but you done command in  prod environment then it will destroy the code in dev environment

6 - what is splat syntax in terraform and element ,functions,

>>Splat expressions have a special behavior when you apply them to a value that isn't a list, set, or tuple. If the value 
is anything other than a null value then the splat expression will transform it into a single-element list, or more accurately a single-element tuple value

7 -  where we use concat and what is concat

>>The CONCAT function combines the text from multiple ranges and/or strings, but it doesn't provide delimiter or IgnoreEmpty arguments. 
CONCAT replaces the CONCATENATE function. However, the CONCATENATE function will stay available for compatibility with earlier versions of Excel

8 - Define null resource in Terraform.
null_resource implements standard resource library, but no further action is taken. The triggers argument allows an arbitrary set of values that 
will cause the replacement of resources when changed

9 - what is local-exec and remote-exec

>>local-exec and remote-exec provisioners. The local-exec provisioner invokes a local executable after a resource is created. 
This invokes a process on the machine running Terraform, not on the resource. The remote-exec provisioner invokes a script on a remote resource after it is created.

10- what is for_each and count how to use in terraform and how many types of variables and what is variables 

>>count is good if there is not change in the list index changes are sequentical/leanier. 
any changes in the existing index will for the resouces to recreate

>>this can be solved but for_each bcoz it uses key rather than index

11 - what is modules in terraform 

>>it is repetable code.one code you can use number of times

12 -  what is terraform life cycle, Imports , terraform cloud 

---Life cycle:-
>>create_before_destroy
--if your created an s3 or any system but your created wrong bucket or wrong vpc this logs have store in vpc flow logs
but you dont want lose that dat on that you use create_before_destroy and then your write a code to
copy the files to some other bucket store the data 

>>prevent_destroy
--if your createdan s3 bucket are any vpc or any ec2 system or anything no one want to delete you can use 
this prevent_destory 

lifecycle {
prevent_destory = true
}

>>ignore_changes
--if deployed some instance using terraform but your changed some of the tags or some data change 
maually but in terraform you need to use ignore_changes then the the cant destory what you changed manually

lifecycle {
ignore changes = [
 tags,enable_dns_hostname
]
}

13 - what is terraform Imports?

>>terraform Imports:-
---when we have mistkaley or already deployed server in ec2 any service you need import that service
using terraform command you can take the code command to copy the the code code into .tf file

commands:-

>>terraform import aws_instance.importing "instance-id from the aws"
>>terraform state list
>>terraform state show aws_instance.importing          -->you will get code copy and paste below your code page

resource "aws_instance" "importing" {

paste that copy that data

}

14 - what is terraform-cloud

>>What is Terraform Cloud? Terraform Cloud is HashiCorp's managed service offering. It eliminates the need for unnecessary tooling and
documentation for practitioners, teams, and organizations to use Terraform in production. Provision infrastructure in a remote environment 
that is optimized for the Terraform workflow.

What is Terraform cloud used for?
Terraform Cloud enables infrastructure automation for provisioning, compliance, and management of any cloud, datacenter, and service

15 - what is packer

>>packer is used to create only images
>>Packer is an open-source VM image creation tool from Hashicorp. 
It helps you automate the process of Virtual machine image creation on the cloud and on-prem virtualized environments.

16 - what is terraform dry-run

>>Dry runs help you identify trouble spots, discover sections you may not fully understand, and get a better understanding of how everything fits together.
It's also a good way to find out if you don't fully understand how an activity is supposed to be conducted or how the virtual tools function.

17 - What are the most useful Terraform commands?
Some of the most useful Terraform commands are:

terraform init - initializes the current directory
terraform refresh - refreshes the state file
terraform output - views Terraform outputs
terraform apply - applies the Terraform code and builds stuff
terraform destroy - destroys what has been built by Terraform
terraform graph - creates a DOT-formatted graph
terraform plan - a dry run to see what Terraform will do

18 - What are some of the built-in provisioners available in Terraform?
Here is the list of built-in provisioners in Terraform:

Salt-masterless Provisioner
Remote-exec Provisioner
Puppet Provisioner
Local-exec Provisioner
Habitat Provisioner
File Provisioner
Chef Provisioner

19 - What is terraform taint and untaint?
Image result for what is terraform taint
The Terraform Taint command allows you to manually flag a resource as tainted, which means it will be destroyed and recreated on the next terraform apply. 
Terraform untaint allows you to remove that tainted condition from the resource

20 - what is the use of provisoners in packer and terraform

>>Terraform Provisioners are used for executing scripts or shell commands on a local or remote machine as part of resource creation/deletion. 
They are similar to “EC2 instance user data” scripts that only run once on the creation and if it fails terraform marks it tainted

===============================================Kubernetes===============================================================================================

1 - what is kubernets why everyone using kubernets
>>Kubernetes is a way to deploy images and containers and manage the scaling, deployment, resource balancing and traffic for cloud services. 
This is becoming essential as teams build microservices and package them into containers
           or
>>if your runing an image in pods but your suddenly changing into image but it will not change or update
immediately . the pods need to chnage old to new pods then you will get . you should delete the pods or
remove the pods and add the pods into replication-controller or replication-set

>>you can generate yaml code using --dry-run -o yaml command using
>>k8s is designed for stateless applications (it is temporaray data) if the pod deleted 
data automatically delete

2 - what is runtime in kubernets
>>Runtime. The container runtime is the low-level component that creates and runs containers. Docker currently uses runC, the most popular runtime, 
which adheres to the OCI standard that defines container image formats and execution

3 - what is the differnce between monolithic and microservices and advantages and disadvantages
>>A monolithic application is built as a single unified unit while a microservices architecture is a collection of smaller, independently deployable services.

What are the advantages and disadvantages of monolithic architecture?
Monolithic systems are systems in which all services and functions are tightly bonded together in a “monolith”. Monolithic architecture has many advantages, 
including speedy development and deployment, but it often lacks flexibility and scalability.

What are Microservices?
Pros of Microservices. Easier Scaling Up. Improved Fault Tolerance. Ease of Understanding of the Codebase. Scope for Experimenting. ...
Cons of Microservices. Increased Complexity of Communication. Requires More Resources. Global Testing and Debugging is Difficult. Not Practical for Small Applications.

4 - what is etcd in kubernets
>>etcd is like an raft database, etcd is an key value pair database
>>etcd is an encypted database , one and only api will connect to the database

what is raft database
>>Raft is a distributed consensus algorithm. It was designed to be easily understood. It solves the problem of getting multiple servers 
to agree on a shared state even in the face of failures. The shared status is usually a data structure supported by a replicated log

5 - what is kube proxy what is the work on kube proxy
>>Kube-proxy maintains network rules on each Amazon EC2 node. It enables network communication to your pods. Kube-proxy is not deployed to Fargate nodes. 
For more information, see kube-proxy in the Kubernetes documentation.

6 - what is kube schedular and what is the work on kube schedular
>>The Kubernetes scheduler is a control plane process which assigns Pods to Nodes. The scheduler determines which Nodes are valid placements for
 each Pod in the scheduling queue according to constraints and available resources

7 - what is kube controller manager 
>>The Kubernetes controller manager is a daemon that embeds the core control loops shipped with Kubernetes. In applications of robotics and automation, 
a control loop is a non-terminating loop that regulates the state of the system

8 - what is kube-api-server
>>Kubernetes API server runs as a container (kube-apiserver) within Pods in the kube-system namespace. In order to make its access easier, 
it's exposed through a service named kubernetes in the default namespace.

9 - what is kubelet 
>>kubelet is an agent of kubernets 
>>What is Kubelet used for?
The kubelet is the primary "node agent" that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; 
or specific logic for a cloud provider. The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod

10 - what is pod in kubernets
>>pod can have multiple containers or multiple volumes this called pods
>>pod is nothing but container .you cant create container in kubernets
>>pod inside if have multiple containers but pod have only one ip address

11 - what is kubernets CNI container network interafce

>>Container Network Interface (CNI) is a framework for dynamically configuring networking resources. It uses a group of libraries and specifications written in Go. 
The plugin specification defines an interface for configuring the network, provisioning IP addresses, and maintaining connectivity with multiple hosts

12 - how to connect your pods local
>>using port forwarding or using services

13 - what is kubenet limitions

>>kubenet - a simple /24 IP address range can support up to 251 nodes in the cluster (each Azure virtual network subnet reserves the 
first three IP addresses for management operations)
 This node count could support up to 27,610 pods (with a default maximum of 110 pods per node with kubenet

14 - what is pod security policy

>>A Pod Security Policy is a cluster-level resource that controls security sensitive aspects of the pod specification. 
The PodSecurityPolicy objects define a set of conditions 
that a pod must run with in order to be accepted into the system, as well as defaults for the related fields.

15 - what is the differnece between lables and annonations

>>Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, 
annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, 
and can include characters not permitted by labels

16 - what is the importanets of lables

>>Kubernetes labels are key-value pairs that can connect identifying metadata with Kubernetes objects. 
Kubernetes offers integrated support for using these labels to query objects and perform bulk operations on selected subsets



17 - what is the differnce between replication controller and replication set and what is replication controller

>>>>replication-controller is used to increase or decrease the pods
>>it is old version it doesnt accept multiple labels that why we use replication-set
>>if your runing an image in pods but your suddenly changing into image but it will not change or updateimmediately . the pods need to chnage 
old to new pods then you will get . you should delete the pods or remove the pods and add the pods into replication-controller or replication-set

>>The major difference between a replication controller and replica set is that the rolling-update command works with Replication Controllers, 
but won't work with a Replica Set

18 - what is metadata

>>Metadata makes finding and working with data easier – allowing the user to sort or locate specific documents. Some examples of basic metadata are author, date created, date modified, and file size. 
Metadata is also used for unstructured data such as images, video, web pages, spreadsheets, etc.

19 - how to remove pods and how to add pods into replicas

>>First, confirm the name of the node you want to remove using kubectl get nodes , and make sure that all of the pods on the node can be safely terminated without any special procedures. 
Next, use the kubectl drain command to evict all user pods from the node

20 - what is rolling update in kubernets and disadvantages

>>Rolling updates incrementally replace your resource's Pods with new ones, which are then scheduled on nodes with available resources. 
Rolling updates are designed to update your workloads without downtime. The following objects represent Kubernetes workloads

disadvantages:-
1 - it will be a manual update from one image to other image
2 - new replication-controller will be created and old replication-controller will be deleted
3 - roll back needs to change again to the old image
4 - Overall manual process and replication-controller update is deprecated

21 - how you can update your pods or services
>>we can use deployments to update the but we don't use replication-controller
>>we use manifest files to version control. then we can see the versioning if we kubectl 
but after week we come what command we used we don't remeber and there is no version control right

22 - what is deployments what is the use of deployments

>>A Kubernetes Deployment tells Kubernetes how to create or modify instances of the pods that hold a containerized application. 
Deployments can help to efficiently scale the number of replica pods, enable the rollout of updated code in a controlled manner, or roll back to an earlier deployment version if necessary.

Advantages:-
1 - it uses replicasets and replicasets automatically performs the rollingupdates.
2 - Rollback is easy as we can record the deployments
3 - we can use liveness and reasyness probes to improve application availibility
4 - we can pause & resume the deployment which us usefull for canary updates

23 - what is liveness probe and readyness probe

Liveness probes determine whether or not an application running in a container is in a healthy state. 
If the liveness probe detects an unhealthy state, then Kubernetes kills the container and tries to redeploy it. The liveness probe is configured in the spec. containers.

A readiness probe indicates whether applications running in a container are ready to receive traffic. 
If so, Services in Kubernetes can send traffic to the pod, and if not, the endpoint controller removes the pod from all services

24 - what is service and types of services

>>A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them. 
Services enable a loose coupling between dependent Pods. A Service is defined using YAML (preferred) or JSON, like all Kubernetes objects

types:-
1 - ClusterIP what is clusterip
2 - Nodeport what is nodeport
3 - loadbalancer what isloadbalancer
4 - None(headless)what is headless service
5 - Ingress controller what is ingress controller 

25 - what is headless services

>>Headless service is a regular Kubernetes service where the spec. clusterIP is explicitly set to "None" and spec. 
type is set to "ClusterIP". Instead, SRV records are created for all the named ports of service's endpoints.

26 - what is ingress controller and advantages

>>An Ingress controller abstracts away the complexity of Kubernetes application traffic routing and provides a bridge between Kubernetes services and external ones. 
Kubernetes Ingress controllers: Accept traffic from outside the Kubernetes platform, and load balance it to pods (containers) running inside the platform.

>>What is one of the benefits of an ingress controller on the cloud?
The major advantage of using a cloud-based Ingress Controller is native integration with other cloud services. 
For instance, GCE Ingress Controller supports Cloud IAP for Google Kubernetes Engine to easily turn on Identity-Aware Proxy to protect internal K8s applications

27 - one ingress controller can you add another ingress or you can ad one ingress to add muliple domains to route

ans:yes  
ans:yes  

28 - what is NameSpaces and what is use for it

>>Namespaces are a way to organize clusters into virtual sub-clusters — they can be helpful when different teams or projects share a Kubernetes cluster. 
Any number of namespaces are supported within a cluster, each logically separated from others but with the ability to communicate with each other

29 - what is resource-quota and limit-range in kubernetes

>>The resource quota is the total allocated resources for a particular namespace, 
while limit range also used to assign limits for objects like containers (Pods) running inside the namespace. This is one of the best practice recommended.

30 - what is user-Acount and Service-Account in kubernetes

>>User accounts are meant to be used by humans. But when a pod running in the cluster wants to access the Kubernetes API server, it needs to use a service account instead. 
Service accounts are just like user accounts but for non-humans

31 - what is api-resources and api-versions

>>They are available in the discovery documentation, i.e. via the API, e.g. /api/apps/v1 . 
where api-version is core for the core resources and should be replaced by "" (an empty quoted string) in your role definition. For example, core pods/status: get patch update

What are API versions in Kubernetes?
The Google Kubernetes Engine API has three groups: v1 for generally available features. v1alpha1 for alpha features. v1beta1 for beta features

32 - what is Role, clusterRole, RoleBinding, ClusterRoleBinding

we can create cluster level but assigning namespace level why because if one user need to access the 
multiple namespaces if we use namespace to create 5 namespaces we need to create 5 roles so that why we use 
cluster role so we can give access to namespace-level 

33 - what is multi-container pods in kubernetes and use-case and types 

>>runing multiple containers in a singel pod is called multi-container pod

what is the use-case:
>>backend jobs need to run we are using to run git sync using emptydir downloading gitrepo
using main container sidecar container 

types:

1-Init containers
>>it will complete work and go 
but if init container not complete then app container not come until the init container complete

2-Sidecar containers
>>sidecar containers will help to app containers

34 - what are the differnet types conatiners we can deploy

1 - init containers
2 - app containers
3 - sidecar containers
 
35 - what is stateless application and stateful application

>>A stateless system sends a request to the server and relays the response (or the state) back without storing any information. On the other hand, 
stateful systems expect a response, track information, and resend the request if no response is received

36 - what is kubernetes volumes. where to use this volumes and how to use

>>A Kubernetes volume is a directory that contains data accessible to containers in a given Pod in the orchestration and scheduling platform. 
Volumes provide a plug-in mechanism to connect ephemeral containers with persistent data stores elsewhere

37 - what is emptydir in kubernetes

>>One very useful aspect of Kubernetes emptyDir volumes is the support for a memory-backed filesystem as the backing store. 
This is very useful for certain scenarios when you want to use the node memory as a local cache or as a means to share data between different containers.

38 - how many types attaching volumes in kubernetes or Raw Disk Mapping

1 - Direct Mapping
--you can directly map ebs raws volume to the pod 

Disadvantages:-
1-you should give volume id to the developer
2-the major drawback is that it can be only mounted to one ec2-instance
3-this also means that it can't be shared between multiple pods
4-only one pod can using raw disk can read-write

2 - Static Provisioning (pv-persistent volume, pvc-persistentvolumecliam)

1-cluster admin must create persistent volume
2-Developer can use the persistent volume using the persistent volume clamis

3 - Dynamic Provisioning(pvc -persistentvolumecliam)

>>Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually 
make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes.

39 - what is persistent volumes && non-persistent volumes

>>A persistent volume is a piece of storage in a cluster that an administrator has provisioned. It is a resource in the cluster, just as a node is a cluster resource. 
A persistent volume is a volume plug-in that has a lifecycle independent of any individual pod that uses the persistent volume.

>>Non-persistent storage: Your data can be removed when the container, the worker node, or the cluster is removed. Non-persistent storage is typically used for logging information, 
such as system logs or container logs, development testing, or when you want to access data from the host's file system

40 - what is the problem or dis-advantages. if we use kubernetes volume as aws-EBS

1-we cannot mount to multiple nodes
2-cannot be used if the app is distrbuted multiple Availability zones
3-mounting takes time for EBS
4-low mount times and stuck volumes which equal slow deployments
5-slow failover, which means no high availability

41 - what is stateful-set and advantages and dis-advantages

>>StatefulSet is the workload API object used to manage stateful applications. Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. 
Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec.

42 - what is daemon-set

>>DaemonSet manages groups of replicated Pods. However, DaemonSets attempt to adhere to a one-Pod-per-node model, either across the entire cluster or a subset of nodes.
 As you add nodes to a node pool, DaemonSets automatically add Pods to the new nodes as needed

43 - what is node-maintenance and uses also

>>Placing a node into maintenance marks the node as unschedulable and drains all the virtual machines and pods from it. 
Virtual machine instances that have a LiveMigrate eviction strategy are live migrated to another node without loss of service

44 - what is advance-scheduling and tiants and tolerance

>>Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). 
Taints are the opposite -- they allow a node to repel a set of pods. Tolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints.

45 - how to save logs of k8s cluster

>>To get Kubectl pod logs, you can access them by adding the -p flag. Kubectl will then get all of the logs stored for the pod. 
This includes lines that were emitted by containers that were terminated.

46 - what is fluentd 

>>Fluentd is a popular open-source data collector that we'll set up on our Kubernetes nodes to tail container log files, 
filter and transform the log data, and deliver it to the Elasticsearch cluster, where it will be indexed and stored

47 - what is node affinity and pod affinity ,anti afffinity

>>Node affinity is one of the mechanisms Kubernetes provides to define where Kubernetes should schedule a pod. 
It lets you define nuanced conditions that influence which Kubernetes nodes are preferred to run a specific pod.

>>Pod affinity/anti-affinity allows you to constrain which nodes your pod is eligible to be scheduled on based on the labels on other pods

48 - what is kubernetes secrets. which type most uses in secrets

>>Secrets are Kubernetes resources used to configure your applications with sensitive data that can be made accessible to your container workloads at runtime.
 Kubernetes has native support for storing and handling this type of sensitive data with care

>>What are the type of secrets in Kubernetes?
Kubernetes Secrets are secure objects which store sensitive data, such as passwords, OAuth tokens, and SSH keys, etc. 
with encryption in your clusters. Using Secrets gives you more flexibility in a Pod Life cycle definition and control over how sensitive data is used.

49 - What is the Blue/Green Deployment Pattern

actually three ways update your pods 
1-spining up new pods and delete old pods

>>we pods connected in our service but we delete the pods and we can create the pods and connect to that services this one method there is a latency and 
downtime in your application

2-blue-green deployment pattern
>>we have one replication controller1 we need update some pods into new image so we can create one more replication controller2 and we can chane service 
replication controller1 to second controller2 directly but we will get some latency downtime in your application to overcome we can use more method

rolling-update:-
if we have service that connected 3 pods we can create 3 more new pods with new image and we can apply rolling update then it will delete the first pod and add 
that first pod place new image pod and delete the second pod and add new image pod and one more add new image pod it will update automatically there is no latency
or downtime in your application

































===========





1 - what is shell scripting in DevOps and what is Ansible and what is difference between shell-script and Ansible in DevOps purpose



2 - what is power-shell



3 - What is the Blue/Green Deployment Pattern


